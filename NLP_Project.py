# -*- coding: utf-8 -*-
"""week3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f56QfUphw0gaEycbSmJy5rmWMvbkjykH
"""

!pip install jedi

!apt-get update -y
!apt-get install -y build-essential python3-dev

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers datasets seqeval scikit-learn matplotlib nltk pdfplumber

import random, time, json
from pathlib import Path
from collections import Counter, defaultdict
import nltk
nltk.download("punkt", quiet=True)
from nltk import word_tokenize
import matplotlib.pyplot as plt
import pandas as pd

random.seed(42)

OUT_DIR = Path("ner_data_quick")
OUT_DIR.mkdir(exist_ok=True)
# Set small_demo=True to run fast and inspect output; set False to generate full sizes.
small_demo = True

if small_demo:
    TRAIN_N, DEV_N, TEST_N = 200, 50, 50
else:
    TRAIN_N, DEV_N, TEST_N = 50000, 5000, 5000

ADD_DOSAGES = True
ADD_TIME_PHRASES = True
ADD_NEGATION = True

patients = ["John Smith", "Mary Johnson", "The patient", "He", "She", "Mr. Brown", "Ms. Davis", "Patient A"]
symptoms = ["fever", "high fever", "chest pain", "shortness of breath", "cough", "nausea", "vomiting", "headache"]
diseases = ["diabetes", "type 2 diabetes", "hypertension", "infection", "influenza", "the flu"]
drugs = ["Aspirin", "Paracetamol", "Ibuprofen", "Metformin", "Insulin", "Amoxicillin", "Azithromycin"]
dosages = ["5 mg", "10 mg", "500 mg", "twice daily", "once a day"]
time_phrases = ["yesterday", "last week", "this morning", "2 days ago"]
negations = ["no", "not", "denies", "without"]

def entity_to_bio(entity, label):
    toks = entity.split()
    tags = [f"B-{label}"] + [f"I-{label}"]*(len(toks)-1)
    return toks, tags

def add_words(tokens, tags, words, tag="O"):
    for w in words:
        tokens.append(w); tags.append(tag)

# A handful of templates for diversity
def gen_one():
    tpl = random.choices(
        ["sym_disease","drug_symptom","history","complex","negation","dosage"],
        [0.30,0.25,0.20,0.15,0.05,0.05])[0]
    tokens, tags = [], []
    p = random.choice(patients)
    s = random.choice(symptoms)
    d = random.choice(diseases)
    dr = random.choice(drugs)
    dose = random.choice(dosages) if ADD_DOSAGES else None
    tp = random.choice(time_phrases) if ADD_TIME_PHRASES else None
    neg = random.choice(negations) if ADD_NEGATION else None

    if tpl == "sym_disease":
        if " " in p: et, tg = entity_to_bio(p,"PATIENT"); add_words(tokens,tags,et); tags[-len(et):]=tg
        else: add_words(tokens,tags,[p],"O")
        add_words(tokens,tags,["has"],"O")
        et, tg = entity_to_bio(s,"SYMPTOM"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["and"],"O")
        et, tg = entity_to_bio(d,"DISEASE"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["."],"O")
    elif tpl == "drug_symptom":
        add_words(tokens,tags,["The","doctor","prescribed"],"O")
        et, tg = entity_to_bio(dr,"DRUG"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["for"],"O")
        et, tg = entity_to_bio(s,"SYMPTOM"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["."],"O")
        if tp and random.random()<0.25: add_words(tokens,tags,[tp],"O")
    elif tpl == "history":
        if " " in p: et, tg = entity_to_bio(p,"PATIENT"); add_words(tokens,tags,et); tags[-len(et):]=tg
        else: add_words(tokens,tags,[p],"O")
        add_words(tokens,tags,["has","a","history","of"],"O")
        et, tg = entity_to_bio(d,"DISEASE"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["and","takes"],"O")
        et, tg = entity_to_bio(dr,"DRUG"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["."],"O")
    elif tpl == "complex":
        if " " in p: et, tg = entity_to_bio(p,"PATIENT"); add_words(tokens,tags,et); tags[-len(et):]=tg
        else: add_words(tokens,tags,[p],"O")
        add_words(tokens,tags,["complained","of"],"O")
        et, tg = entity_to_bio(s,"SYMPTOM"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,[",","was","diagnosed","with"],"O")
        et, tg = entity_to_bio(d,"DISEASE"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["and","treated","with"],"O")
        et, tg = entity_to_bio(dr,"DRUG"); add_words(tokens,tags,et); tags[-len(et):]=tg
        if dose and random.random()<0.3:
            dtoks = dose.split(); add_words(tokens,tags,dtoks,"O")
        add_words(tokens,tags,["."],"O")
    elif tpl == "negation":
        if " " in p: et, tg = entity_to_bio(p,"PATIENT"); add_words(tokens,tags,et); tags[-len(et):]=tg
        else: add_words(tokens,tags,[p],"O")
        add_words(tokens,tags,[neg],"O")
        et, tg = entity_to_bio(s,"SYMPTOM"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["."],"O")
    else:  # dosage
        add_words(tokens,tags,["Gave"],"O")
        if dose:
            dtoks = dose.split(); add_words(tokens,tags,dtoks,"O")
        add_words(tokens,tags,["of"],"O")
        et, tg = entity_to_bio(dr,"DRUG"); add_words(tokens,tags,et); tags[-len(et):]=tg
        if random.random()<0.6:
            add_words(tokens,tags,["for"],"O"); et, tg = entity_to_bio(s,"SYMPTOM"); add_words(tokens,tags,et); tags[-len(et):]=tg
        add_words(tokens,tags,["."],"O")
    # final optional time phrase
    if ADD_TIME_PHRASES and random.random()<0.05:
        add_words(tokens,tags,[tp],"O")
    return tokens, tags

def make_set(n, name):
    examples = []
    batch = 1000 if n>2000 else 100
    t0 = time.time()
    for i in range(n):
        examples.append(gen_one())
        if (i+1) % batch == 0:
            print(f"{name}: generated {i+1}/{n} examples ... {time.time()-t0:.1f}s")
    print(f"{name}: done ({n} examples, {time.time()-t0:.1f}s)")
    return examples

print("Starting generation (small_demo=%s)..." % small_demo)
train = make_set(TRAIN_N,"TRAIN")
dev   = make_set(DEV_N,"DEV")
test  = make_set(TEST_N,"TEST")

def write_conll(exs, path):
    with open(path,"w",encoding="utf-8") as f:
        for tokens,tags in exs:
            for t,tag in zip(tokens,tags):
                f.write(f"{t} {tag}\n")
            f.write("\n")
write_conll(train, OUT_DIR/"train.conll")
write_conll(dev,   OUT_DIR/"dev.conll")
write_conll(test,  OUT_DIR/"test.conll")
print("Wrote files to", OUT_DIR.resolve())

def show_samples(exs, k=5):
    print("\n--- Sample sentences ---")
    for i in range(min(k,len(exs))):
        toks,tags = exs[i]
        print(f"{i+1})", " ".join(toks))
        print("   ", " ".join(tags))

show_samples(train, k=6)

# Label distribution
counter = Counter()
for ex in train:
    counter.update(ex[1])
label_df = pd.DataFrame(counter.most_common(), columns=["label","count"])
print("\n--- Label counts (train) ---")
print(label_df.head(20).to_string(index=False))

# Bar chart
plt.figure(figsize=(8,4))
plt.bar(label_df['label'].iloc[:12], label_df['count'].iloc[:12])
plt.xticks(rotation=45, ha='right')
plt.title("Top label counts (train)")
plt.tight_layout()
plt.show()

# Top entities (by text) across train
ent_counts = Counter()
for toks,tags in train:
    cur = []
    cur_type = None
    for w,tag in zip(toks,tags):
        if tag=="O":
            if cur:
                ent_counts[(cur_type, " ".join(cur))]+=1
                cur=[]; cur_type=None
            continue
        if tag.startswith("B-"):
            if cur:
                ent_counts[(cur_type, " ".join(cur))]+=1
            cur=[w]; cur_type=tag[2:]
        elif tag.startswith("I-") and cur_type:
            cur.append(w)
    if cur:
        ent_counts[(cur_type, " ".join(cur))]+=1

print("\n--- Top 25 entities (type, text, count) ---")
for (typ, txt), c in ent_counts.most_common(25):
    print(f"{typ:8s} | {txt:25s} | {c}")

# Save metadata
meta = dict(train_n=TRAIN_N, dev_n=DEV_N, test_n=TEST_N, small_demo=small_demo, timestamp=time.time())
with open(OUT_DIR/"metadata.json","w",encoding="utf-8") as f:
    json.dump(meta,f,indent=2)
print("\nMetadata saved.")

# --- Quick test case for your NER generator ---

# Generate 1 sample
sample_tokens, sample_tags = gen_one()

print("=== SAMPLE SENTENCE ===")
print(" ".join(sample_tokens))

print("\n=== BIO TAGS ===")
print(" ".join(sample_tags))

# Check length match
print("\nLength check:")
print("Tokens:", len(sample_tokens), " Tags:", len(sample_tags))

# Extract entity spans for verification
entities = []
current = []
ent_type = None

for tok, tag in zip(sample_tokens, sample_tags):
    if tag == "O":
        if current:
            entities.append((ent_type, " ".join(current)))
            current = []
            ent_type = None
        continue

    if tag.startswith("B-"):
        if current:
            entities.append((ent_type, " ".join(current)))
        current = [tok]
        ent_type = tag.split("-")[1]

    elif tag.startswith("I-"):
        if ent_type == tag.split("-")[1]:
            current.append(tok)
        else:
            # BIO inconsistency fix
            entities.append((ent_type, " ".join(current)))
            current = [tok]
            ent_type = tag.split("-")[1]

if current:
    entities.append((ent_type, " ".join(current)))

print("\n=== Extracted Entities ===")
for etype, text in entities:
    print(f"{etype:8s} | {text}")

!pip install -q transformers datasets sentencepiece accelerate

"""WEEK 4"""

# summarise_and_evaluate.py

from transformers import pipeline
import re, json

# 1. summarizer (pretrained, no training)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# 2. VERY simple NER stub â€“ replace with your Week-3/BioBERT NER
def simple_ner(text):
    # TODO: call your trained NER model here
    tokens = re.findall(r"\b[A-Z][a-zA-Z0-9\-]+\b", text)
    return list(set(tokens))

def extract_entities_from_list(text, entity_list):
    found = []
    t = text.lower()
    for e in entity_list:
        if re.search(r'\b' + re.escape(e.lower()) + r'\b', t):
            found.append(e)
    return found

def evaluate_example(source, summary):
    src_ents = simple_ner(source)
    sum_ents = simple_ner(summary)
    # coverage: entities from source that appear in summary
    covered = [e for e in src_ents if e in sum_ents]
    missing = [e for e in src_ents if e not in sum_ents]
    hallucinated = [e for e in sum_ents if e not in src_ents]
    return {
        "src_entities": src_ents,
        "sum_entities": sum_ents,
        "covered": covered,
        "missing": missing,
        "hallucinated": hallucinated
    }

# ---- demo with a few examples ----
examples = [
    {
        "id": "1",
        "source": "The patient with hypertension and diabetes was started on aspirin and metformin. Blood pressure improved.",
    },
    {
        "id": "2",
        "source": "Chest pain patient with elevated troponin, ECG changes, treated with heparin and aspirin.",
    },
]

results = []
for ex in examples:
    raw = summarizer(ex["source"], max_length=60, min_length=10, do_sample=False)[0]["summary_text"]
    metrics = evaluate_example(ex["source"], raw)
    results.append({
        "id": ex["id"],
        "source": ex["source"],
        "summary": raw,
        **metrics
    })

print(json.dumps(results, indent=2))

"""TRAINING"""

# === CLEAN WORKING CoNLL PARSER â†’ HF DATASET ===
from pathlib import Path
from datasets import Dataset, DatasetDict
import json
import itertools

BASE = Path("/content/ner_data_quick")

def read_conll_file(path):
    """Reads a .conll file and yields dicts: {"tokens": [...], "ner_tags":[...]}"""
    sentences = []
    tokens, tags = [], []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                if tokens:
                    sentences.append({"tokens": tokens, "ner_tags": tags})
                    tokens, tags = [], []
                continue
            parts = line.split()
            token = parts[0]
            tag = parts[-1]
            tokens.append(token)
            tags.append(tag)
    if tokens:
        sentences.append({"tokens": tokens, "ner_tags": tags})
    return sentences

train_examples = read_conll_file(BASE/"train.conll")
dev_examples   = read_conll_file(BASE/"dev.conll")
test_examples  = read_conll_file(BASE/"test.conll")

print("Loaded:",
      "train =", len(train_examples),
      "dev =", len(dev_examples),
      "test =", len(test_examples))

# Build label list from all tags across all splits
all_tags = sorted(list({
    tag
    for example in train_examples + dev_examples + test_examples
    for tag in example["ner_tags"]
}))
print("Label list:", all_tags)

label_to_id = {label:i for i, label in enumerate(all_tags)}

# Convert tag strings â†’ tag IDs
def convert_tags(example):
    return {
        "tokens": example["tokens"],
        "ner_tags": [label_to_id[t] for t in example["ner_tags"]]
    }

train_ds = Dataset.from_list([convert_tags(e) for e in train_examples])
dev_ds   = Dataset.from_list([convert_tags(e) for e in dev_examples])
test_ds  = Dataset.from_list([convert_tags(e) for e in test_examples])

dataset = DatasetDict({
    "train": train_ds,
    "validation": dev_ds,
    "test": test_ds
})

print(dataset)

# Save labels for later use
with open("/content/ner_label_list.json", "w") as f:
    json.dump(all_tags, f, indent=2)

print("Saved label list to /content/ner_label_list.json")

# @title
# === TOKENIZE + ALIGN NER TAGS ===
from transformers import AutoTokenizer
import numpy as np

label_list = json.load(open("/content/ner_label_list.json"))
label_to_id = {l:i for i,l in enumerate(label_list)}

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=True)

def tokenize_and_align_labels(batch):
    tokenized = tokenizer(
        batch["tokens"],
        is_split_into_words=True,
        truncation=True
    )
    labels = []
    for i, ner_tags in enumerate(batch["ner_tags"]):
        word_ids = tokenized.word_ids(batch_index=i)
        label_ids = []
        prev = None
        for w in word_ids:
            if w is None:
                label_ids.append(-100)
            else:
                if w != prev:
                    label_ids.append(ner_tags[w])
                else:
                    label_ids.append(-100)
            prev = w
        labels.append(label_ids)
    tokenized["labels"] = labels
    return tokenized

tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)
tokenized_datasets

!pip install -q transformers datasets seqeval sentencepiece

!pip install -q evaluate

import os
os.kill(os.getpid(), 9)

import json
label_list = json.load(open("/content/ner_label_list.json"))

from pathlib import Path
import random, time, json
import nltk
nltk.download("punkt", quiet=True)

OUT_DIR = Path("/content/ner_data_quick")
OUT_DIR.mkdir(exist_ok=True)

# dataset sizes (small for testing)
TRAIN_N, DEV_N, TEST_N = 200, 50, 50

patients = ["John Smith","Mary Johnson","The patient","He","She","Mr. Brown","Ms. Davis","Patient A"]
symptoms = ["fever","high fever","chest pain","shortness of breath","cough","nausea","vomiting","headache"]
diseases = ["diabetes","type 2 diabetes","hypertension","infection","influenza","the flu"]
drugs    = ["Aspirin","Paracetamol","Ibuprofen","Metformin","Insulin","Amoxicillin","Azithromycin"]
dosages  = ["5 mg","10 mg","500 mg","twice daily","once a day"]
time_phrases = ["yesterday","last week","this morning","2 days ago"]
negations = ["no","not","denies","without"]

def entity_to_bio(entity, label):
    toks = entity.split()
    tags = ["B-"+label] + ["I-"+label]*(len(toks)-1)
    return toks, tags

def add(tokens, tags, words, tag="O"):
    for w in words:
        tokens.append(w); tags.append(tag)

def gen_one():
    tokens, tags = [], []
    p = random.choice(patients)
    s = random.choice(symptoms)
    d = random.choice(diseases)
    dr = random.choice(drugs)
    dose = random.choice(dosages)
    tp = random.choice(time_phrases)
    neg = random.choice(negations)

    # simple template
    et, tg = entity_to_bio(p, "PATIENT"); add(tokens,tags,et); tags[-len(et):]=tg # FIX: Correctly assign BIO tags
    add(tokens,tags,["has"])
    et, tg = entity_to_bio(s,"SYMPTOM"); add(tokens,tags,et); tags[-len(et):]=tg # FIX: Correctly assign BIO tags
    add(tokens,tags,["and"])
    et, tg = entity_to_bio(d,"DISEASE"); add(tokens,tags,et); tags[-len(et):]=tg # FIX: Correctly assign BIO tags
    add(tokens,tags,[ "."])

    return tokens, tags

def write_conll(exs, path):
    with open(path,"w") as f:
        for toks,tags in exs:
            for t,tag in zip(toks,tags):
                f.write(f"{t} {tag}\n")
            f.write("\n")

train = [gen_one() for _ in range(TRAIN_N)]
dev   = [gen_one() for _ in range(DEV_N)]
test  = [gen_one() for _ in range(TEST_N)]

write_conll(train, OUT_DIR/"train.conll")
write_conll(dev,   OUT_DIR/"dev.conll")
write_conll(test,  OUT_DIR/"test.conll")

print("NER dataset created at /content/ner_data_quick")

from pathlib import Path
import json

BASE = Path("/content/ner_data_quick")

def read_conll(path):
    sentences = []
    tokens, tags = [], []
    for line in open(path):
        line = line.strip()
        if not line:
            if tokens:
                sentences.append({"tokens":tokens,"ner_tags":tags})
                tokens, tags = [], []
            continue
        parts = line.split()
        token = " ".join(parts[:-1])
        tag   = parts[-1]
        tokens.append(token)
        tags.append(tag)
    if tokens:
        sentences.append({"tokens":tokens,"ner_tags":tags})
    return sentences

train_ex = read_conll(BASE/"train.conll")
dev_ex   = read_conll(BASE/"dev.conll")
test_ex  = read_conll(BASE/"test.conll")

print("Loaded:", len(train_ex), len(dev_ex), len(test_ex))

from datasets import Dataset, DatasetDict

all_tags = sorted(list({t for ex in (train_ex+dev_ex+test_ex) for t in ex["ner_tags"]}))
label_to_id = {t:i for i,t in enumerate(all_tags)}

def convert(ex):
    return {
        "tokens": ex["tokens"],
        "ner_tags": [label_to_id[t] for t in ex["ner_tags"]]
    }

dataset = DatasetDict({
    "train": Dataset.from_list([convert(e) for e in train_ex]),
    "validation": Dataset.from_list([convert(e) for e in dev_ex]),
    "test": Dataset.from_list([convert(e) for e in test_ex]),
})

with open("/content/ner_label_list.json","w") as f:
    json.dump(all_tags,f)

print(dataset)
print("Labels:", all_tags)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=True)

def tokenize_align(batch):
    tokenized = tokenizer(batch["tokens"], is_split_into_words=True, truncation=True)
    new_labels = []
    for i, labels in enumerate(batch["ner_tags"]):
        word_ids = tokenized.word_ids(i)
        label_ids = []
        prev = None
        for w in word_ids:
            if w is None:
                label_ids.append(-100)
            else:
                label_ids.append(labels[w] if w != prev else -100)
            prev = w
        new_labels.append(label_ids)
    tokenized["labels"] = new_labels
    return tokenized

tokenized_datasets = dataset.map(tokenize_align, batched=True)
print("Tokenization done.")

from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)

import os, numpy as np, json
os.environ["WANDB_DISABLED"] = "true"

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification
from seqeval.metrics import f1_score, precision_score, recall_score

label_list = json.load(open("/content/ner_label_list.json"))

def compute_metrics(p):
    preds, labs = p
    pred_ids = np.argmax(preds, axis=2)

    t_preds, t_labs = [], []
    for pr, lb in zip(pred_ids, labs):
        cp, cl = [], []
        for p_i, l_i in zip(pr, lb):
            if l_i != -100:
                cp.append(label_list[p_i])
                cl.append(label_list[l_i])
        t_preds.append(cp)
        t_labs.append(cl)

    return {
        "precision": precision_score(t_labs, t_preds),
        "recall": recall_score(t_labs, t_preds),
        "f1": f1_score(t_labs, t_preds),
    }

model = AutoModelForTokenClassification.from_pretrained(
    "bert-base-cased",
    num_labels=len(label_list)
)

args = TrainingArguments(
    output_dir="/content/ner_model",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    logging_steps=20,
    report_to="none"
)

data_collator = DataCollatorForTokenClassification(tokenizer)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

import os, numpy as np, json
os.environ["WANDB_DISABLED"] = "true"

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification
from seqeval.metrics import f1_score, precision_score, recall_score

label_list = json.load(open("/content/ner_label_list.json"))

def compute_metrics(p):
    preds, labs = p
    pred_ids = np.argmax(preds, axis=2)

    t_preds, t_labs = [], []
    for pr, lb in zip(pred_ids, labs):
        cp, cl = [], []
        for p_i, l_i in zip(pr, lb):
            if l_i != -100:
                cp.append(label_list[p_i])
                cl.append(label_list[l_i])
        t_preds.append(cp)
        t_labs.append(cl)

    return {
        "precision": precision_score(t_labs, t_preds),
        "recall": recall_score(t_labs, t_preds),
        "f1": f1_score(t_labs, t_preds),
    }

model = AutoModelForTokenClassification.from_pretrained(
    "bert-base-cased",
    num_labels=len(label_list)
)

args = TrainingArguments(
    output_dir="/content/ner_model",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    logging_steps=20,
    report_to="none"
)

data_collator = DataCollatorForTokenClassification(tokenizer)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()
trainer.save_model("/content/ner_model")
tokenizer.save_pretrained("/content/ner_model")

print("ðŸŽ‰ Training complete and model saved!")

from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

ner_model = AutoModelForTokenClassification.from_pretrained("/content/ner_model")
ner_tokenizer = AutoTokenizer.from_pretrained("/content/ner_model")

id2label = ner_model.config.id2label

def extract_entities(text):
    tokens = ner_tokenizer(text, return_tensors="pt", truncation=True)
    outputs = ner_model(**tokens)
    preds = outputs.logits.argmax(dim=-1)[0]

    entities = []
    words = ner_tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])

    for word, tag_id in zip(words, preds.tolist()):
        tag = id2label[str(tag_id)]
        if tag.startswith("B-"):
            entities.append(word.replace("â–",""))

    return entities

import random

def make_summary_with_entities(text):
    ents = extract_entities(text)
    # naive baseline summary for training
    summary = "Patient with " + ", ".join(ents) + "."
    return ents, summary

def make_training_example(text):
    entities, summary = make_summary_with_entities(text)
    prompt = f"Document: {text}\nEntities: {', '.join(entities)}\nSummary:"
    return {"input": prompt, "summary": summary}

def extract_entities(text):
    tokens = ner_tokenizer(text, return_tensors="pt", truncation=True)
    outputs = ner_model(**tokens)
    preds = outputs.logits.argmax(dim=-1)[0]

    entities = []
    words = ner_tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])

    for word, tag_id in zip(words, preds.tolist()):
        tag = id2label[tag_id]   # FIXED: no more str(tag_id)
        if tag.startswith("B-"):
            entities.append(word.replace("â–",""))

    return entities

from datasets import Dataset

def conll_to_texts(path):
    texts = []
    tokens = []
    for line in open(path):
        line = line.strip()
        if not line:
            if tokens:
                texts.append(" ".join(tokens))
                tokens = []
            continue
        token = " ".join(line.split()[:-1])
        tokens.append(token)
    return texts

train_texts = conll_to_texts("/content/ner_data_quick/train.conll")
dev_texts   = conll_to_texts("/content/ner_data_quick/dev.conll")

train_data = [make_training_example(t) for t in train_texts]
dev_data   = [make_training_example(t) for t in dev_texts]

dataset = Dataset.from_list(train_data).train_test_split(test_size=0.2)
valset  = Dataset.from_list(dev_data)

dataset

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "t5-small"
summ_tokenizer = AutoTokenizer.from_pretrained(model_name)
summ_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def tokenize_fn(batch):
    model_inputs = summ_tokenizer(
        batch["input"], max_length=512, truncation=True
    )
    with summ_tokenizer.as_target_tokenizer():
        labels = summ_tokenizer(
            batch["summary"], max_length=64, truncation=True
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized = dataset.map(tokenize_fn, batched=True)

from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq

args = TrainingArguments(
    output_dir="/content/t5_entity_summ",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    learning_rate=5e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=20,
    report_to="none",
)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=summ_tokenizer,
    model=summ_model
)

trainer = Trainer(
    model=summ_model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=summ_tokenizer,
    data_collator=data_collator,
)

trainer.train()
trainer.save_model("/content/t5_entity_summ")
summ_tokenizer.save_pretrained("/content/t5_entity_summ")

device = "cuda" if torch.cuda.is_available() else "cpu"
summ_model.to(device)

def summarize_with_entities(text):
    ents = extract_entities(text)

    prompt = f"Document: {text}\nEntities: {', '.join(ents)}\nSummary:"

    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True)

    # FIX: move input tensors to the same device as the model
    inputs = {k: v.to(device) for k, v in inputs.items()}

    output = summ_model.generate(**inputs, max_length=64)
    return summ_tokenizer.decode(output[0], skip_special_tokens=True)

import os
import json
from typing import List, Dict

import torch
from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
)

device = "cuda" if torch.cuda.is_available() else "cpu"

NER_DIR = "/content/ner_model"
SUMM_DIR = "/content/t5_entity_summ"

assert os.path.isdir(NER_DIR), f"NER model folder not found at {NER_DIR}"
assert os.path.isdir(SUMM_DIR), f"Summarization model folder not found at {SUMM_DIR}"

# NER model
ner_model = AutoModelForTokenClassification.from_pretrained(
    NER_DIR,
    local_files_only=True
)
ner_tokenizer = AutoTokenizer.from_pretrained(
    NER_DIR,
    local_files_only=True
)
ner_model.to(device)
ner_model.eval()

# Robust id2label (can be list or dict)
raw_id2label = ner_model.config.id2label
if isinstance(raw_id2label, dict):
    id2label = {int(k): v for k, v in raw_id2label.items()}
else:
    # assume list
    id2label = {i: lab for i, lab in enumerate(raw_id2label)}

# Summarization model (T5)
summ_model = AutoModelForSeq2SeqLM.from_pretrained(
    SUMM_DIR,
    local_files_only=True
)
summ_tokenizer = AutoTokenizer.from_pretrained(
    SUMM_DIR,
    local_files_only=True
)
summ_model.to(device)
summ_model.eval()

print("âœ… Week 6: NER and summarization models loaded.")

def extract_entities(text: str) -> List[str]:
    """
    Run the trained NER model and return entity strings.
    This groups contiguous B-/I- tokens of same type.
    """
    inputs = ner_tokenizer(text, return_tensors="pt", truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = ner_model(**inputs)
        pred_ids = outputs.logits.argmax(dim=-1)[0].tolist()

    tokens = ner_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    entities = []
    current_tokens = []
    current_type = None

    def clean_token(tok: str) -> str:
        # handle BERT wordpieces like ##pain, and sentencepiece underscores
        tok = tok.replace("â–", "")
        if tok.startswith("##"):
            tok = tok[2:]
        return tok

    for tok, tag_id in zip(tokens, pred_ids):
        tag = id2label[tag_id]
        if isinstance(tag, bytes):
            tag = tag.decode("utf-8")

        if tag == "O" or tag is None:
            if current_tokens:
                entities.append(" ".join(current_tokens))
                current_tokens = []
                current_type = None
            continue

        prefix, ent_type = tag.split("-", 1)

        if prefix == "B":
            # start of a new entity
            if current_tokens:
                entities.append(" ".join(current_tokens))
            current_tokens = [clean_token(tok)]
            current_type = ent_type
        elif prefix == "I" and current_type == ent_type:
            current_tokens.append(clean_token(tok))
        else:
            # inconsistent tag â€“ close old entity, start new
            if current_tokens:
                entities.append(" ".join(current_tokens))
            current_tokens = [clean_token(tok)]
            current_type = ent_type

    if current_tokens:
        entities.append(" ".join(current_tokens))

    # remove empties and duplicates
    entities = [e for e in entities if e.strip()]
    return sorted(set(entities))

def evaluate_summary_entities(source: str, summary: str) -> Dict[str, List[str]]:
    """
    Compare entities in source vs summary using the trained NER model.
    """
    src_ents = sorted(set(extract_entities(source)))
    sum_ents = sorted(set(extract_entities(summary)))

    covered = [e for e in src_ents if e in sum_ents]
    missing = [e for e in src_ents if e not in sum_ents]
    hallucinated = [e for e in sum_ents if e not in src_ents]

    return {
        "src_entities": src_ents,
        "sum_entities": sum_ents,
        "covered": covered,
        "missing": missing,
        "hallucinated": hallucinated,
    }


def pretty_print_eval(source: str, summary: str, metrics: Dict):
    print("=" * 80)
    print("SOURCE:")
    print(source)
    print("\nSUMMARY:")
    print(summary)
    print("\nENTITY EVALUATION:")
    print(json.dumps(metrics, indent=2))
    print("=" * 80)

def build_entity_prompt(text: str, ents: List[str]) -> str:
    ents_str = ", ".join(ents) if ents else "none"
    return f"Document: {text}\nEntities: {ents_str}\nSummary:"


def generate_summary_from_prompt(
    prompt: str,
    max_length: int = 64,
    num_beams: int = 1,
    do_sample: bool = False,
    no_repeat_ngram_size: int = 0,
) -> str:
    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    gen_kwargs = {
        "max_length": max_length,
        "num_beams": num_beams,
        "do_sample": do_sample,
    }
    if no_repeat_ngram_size > 0:
        gen_kwargs["no_repeat_ngram_size"] = no_repeat_ngram_size

    output_ids = summ_model.generate(**inputs, **gen_kwargs)
    return summ_tokenizer.decode(output_ids[0], skip_special_tokens=True)


def summarize_with_entities(text: str) -> str:
    """
    Simple entity-aware summarization (no mitigation).
    """
    ents = extract_entities(text)
    prompt = build_entity_prompt(text, ents)
    return generate_summary_from_prompt(
        prompt,
        max_length=64,
        num_beams=1,
        do_sample=False,
    )

def make_safe_fallback_summary(source: str, covered_entities: List[str]) -> str:
    """
    Very conservative backup summary â€“ only uses entities known to be in the source.
    """
    if not covered_entities:
        return "The patient has clinical findings described in the document."
    return "The patient has " + ", ".join(covered_entities) + "."


def summarize_with_mitigation(text: str, max_regenerations: int = 1) -> Dict:
    """
    Full Week 6: entity-guided summarization + hallucination mitigation.

    Returns dict:
      - summary: final summary text
      - strategy: 'baseline' | 'regenerated' | 'fallback'
      - attempts: number of generation attempts
      - metrics: entity coverage/hallucination info
    """
    # 1) Extract entities from source once
    src_entities = sorted(set(extract_entities(text)))

    def one_pass(num_beams=1, no_repeat_ngram_size=0, max_length=64):
        prompt = build_entity_prompt(text, src_entities)
        summary = generate_summary_from_prompt(
            prompt,
            max_length=max_length,
            num_beams=num_beams,
            do_sample=False,
            no_repeat_ngram_size=no_repeat_ngram_size,
        )
        metrics = evaluate_summary_entities(text, summary)
        return summary, metrics

    # First attempt: simple decoding
    summary, metrics = one_pass(num_beams=1, no_repeat_ngram_size=0)
    attempts = 1

    if len(metrics["hallucinated"]) == 0:
        return {
            "summary": summary,
            "strategy": "baseline",
            "attempts": attempts,
            "metrics": metrics,
        }

    # Regenerate with stricter decoding to reduce hallucinations
    best_summary, best_metrics = summary, metrics

    for _ in range(max_regenerations):
        attempts += 1
        regenerated_summary, regenerated_metrics = one_pass(
            num_beams=4,
            no_repeat_ngram_size=3,
            max_length=60,
        )

        # keep the better one (fewer hallucinations)
        if len(regenerated_metrics["hallucinated"]) <= len(best_metrics["hallucinated"]):
            best_summary, best_metrics = regenerated_summary, regenerated_metrics

        if len(regenerated_metrics["hallucinated"]) == 0:
            return {
                "summary": regenerated_summary,
                "strategy": "regenerated",
                "attempts": attempts,
                "metrics": regenerated_metrics,
            }

    # Still hallucinating â†’ fallback safe template
    fallback_summary = make_safe_fallback_summary(text, best_metrics["covered"])
    fallback_metrics = evaluate_summary_entities(text, fallback_summary)

    return {
        "summary": fallback_summary,
        "strategy": "fallback",
        "attempts": attempts,
        "metrics": fallback_metrics,
    }


print("âœ… Week 6 code (verification + mitigation) is ready.")

global id2label
id2label = {i: label for i, label in enumerate(label_list)}

examples = [
    "The patient with hypertension and diabetes was started on aspirin and metformin. Blood pressure improved.",
    "Chest pain patient with elevated troponin, ECG changes, treated with heparin and aspirin.",
    "The patient denies fever or cough but reports headache and nausea. Ibuprofen was given yesterday."
]

for i, src in enumerate(examples, start=1):
    result = summarize_with_mitigation(src, max_regenerations=1)
    pretty_print_eval(src, result["summary"], result["metrics"])
    print("Strategy:", result["strategy"])
    print("Attempts:", result["attempts"])

text = "The patient with hypertension and diabetes was started on aspirin and metformin. Blood pressure improved."
print(extract_entities(text))

res = summarize_with_mitigation(text, max_regenerations=1)
print(res["summary"])
print(res["strategy"])
print(res["metrics"])

!pip install gradio

# =========================
# WEEK 7: POST-PROCESSING
# =========================

import re

def clean_summary_text(text: str) -> str:
    """
    Simple post-processing:
    - strip leading/trailing spaces
    - collapse multiple spaces
    - capitalise first letter
    - ensure it ends with a period
    """
    if not text:
        return text

    # remove extra spaces
    text = text.strip()
    text = re.sub(r"\s+", " ", text)

    # capitalise first non-space character
    if len(text) > 0:
        text = text[0].upper() + text[1:]

    # ensure it ends with .!? (otherwise add a period)
    if text[-1] not in ".!?":
        text = text + "."

    return text


def summarize_with_postprocessing(text: str, max_regenerations: int = 1):
    """
    Wraps summarize_with_mitigation and then cleans the summary.
    Keeps both raw and cleaned versions.
    """
    base = summarize_with_mitigation(text, max_regenerations=max_regenerations)

    raw_summary = base["summary"]
    cleaned = clean_summary_text(raw_summary)

    base["summary_raw"] = raw_summary
    base["summary"] = cleaned
    return base

print("âœ… Week 7: post-processing functions ready.")

# =========================
# WEEK 7: ENTITY HIGHLIGHTING
# =========================

from html import escape

ENTITY_COLORS = {
    "patient": "#d1c4e9",   # light purple
    "symptom": "#ffcdd2",   # light red
    "disease": "#ffe0b2",   # light orange
    "drug": "#bbdefb",      # light blue
    "other": "#c8e6c9",     # light green
}

def infer_entity_type(ent: str) -> str:
    e = ent.lower()
    if e in DEMO_PATIENTS:
        return "patient"
    if e in DEMO_SYMPTOMS:
        return "symptom"
    if e in DEMO_DISEASES:
        return "disease"
    if e in DEMO_DRUGS:
        return "drug"
    return "other"


def highlight_entities_html(text: str, entities: list) -> str:
    """
    Returns HTML string with entities highlighted.
    Uses simple regex replacement; assumes entities don't heavily overlap.
    """
    import re

    # Work on escaped text so HTML is safe
    html_text = escape(text)

    # sort longer phrases first so they don't get split by shorter ones
    entities_sorted = sorted(set(entities), key=len, reverse=True)

    for ent in entities_sorted:
        if not ent.strip():
            continue
        ent_type = infer_entity_type(ent)
        color = ENTITY_COLORS.get(ent_type, ENTITY_COLORS["other"])

        pattern = r"\b" + re.escape(ent) + r"\b"
        span = (
            f'<span style="background-color:{color};'
            f' padding:1px 3px; border-radius:3px; font-weight:bold;">{escape(ent)}</span>'
        )
        html_text = re.sub(pattern, span, html_text, flags=re.IGNORECASE)

    return html_text

def web_demo(text):
    if not text or text.strip() == "":
        return "No text provided.", "", "", "", "", ""

    # entities from source (rule-based)
    src_ents = extract_entities(text)

    # run Week-6+7
    result = summarize_with_postprocessing(text, max_regenerations=1)
    summary = result["summary"]
    strategy = result.get("strategy", "unknown")
    metrics = result.get("metrics", {})

    covered = ", ".join(metrics.get("covered", [])) or "(none)"
    missing = ", ".join(metrics.get("missing", [])) or "(none)"
    hallucinated = ", ".join(metrics.get("hallucinated", [])) or "(none)"
    metrics_json = json.dumps(metrics, indent=2)

    # highlighted HTML
    src_html = highlight_entities_html(text, src_ents)
    sum_html = highlight_entities_html(summary, extract_entities(summary))

    return (
        summary,
        strategy,
        ", ".join(src_ents) or "(none)",
        metrics_json,
        f"Covered: {covered}\nMissing: {missing}\nHallucinated: {hallucinated}",
        src_html,
        sum_html,
    )

# =========================
# WEEK 7: AUTOMATIC EVALUATION
# =========================

import pandas as pd

def evaluate_on_texts(texts, max_regenerations: int = 1) -> pd.DataFrame:
    """
    Runs summarize_with_postprocessing on a list of texts and computes:
      - #src entities / #sum entities
      - #covered / #missing / #hallucinated
      - coverage_rate  = covered / src_entities
      - halluc_rate    = hallucinated / sum_entities
    Returns a pandas DataFrame.
    """
    rows = []

    for i, src in enumerate(texts, start=1):
        res = summarize_with_postprocessing(src, max_regenerations=max_regenerations)
        m = res["metrics"]

        src_ents = m.get("src_entities", [])
        sum_ents = m.get("sum_entities", [])
        covered  = m.get("covered", [])
        missing  = m.get("missing", [])
        halluc   = m.get("hallucinated", [])

        rows.append({
            "id": i,
            "source": src,
            "summary": res["summary"],
            "strategy": res.get("strategy", "unknown"),
            "src_entity_count": len(src_ents),
            "sum_entity_count": len(sum_ents),
            "covered_count": len(covered),
            "missing_count": len(missing),
            "hallucinated_count": len(halluc),
        })

    df = pd.DataFrame(rows)

    # avoid division by zero
    df["coverage_rate"] = df["covered_count"] / df["src_entity_count"].replace(0, 1)
    df["hallucination_rate"] = df["hallucinated_count"] / df["sum_entity_count"].replace(0, 1)

    print("=== Aggregate metrics (entity-level) ===")
    print("Mean coverage rate:      ", df["coverage_rate"].mean())
    print("Mean hallucination rate: ", df["hallucination_rate"].mean())
    print("\nStrategy counts:")
    print(df["strategy"].value_counts())

    return df

print("âœ… Week 7: automatic evaluation ready.")

text = "The patient with hypertension and diabetes was started on aspirin and metformin. Blood pressure improved."

res = summarize_with_postprocessing(text)

print("RAW SUMMARY:", res["summary_raw"])
print("CLEANED SUMMARY:", res["summary"])
print("STRATEGY:", res["strategy"])
print("METRICS:", res["metrics"])

from IPython.display import HTML, display

src = "The patient with hypertension was given aspirin."
res = summarize_with_postprocessing(src)
src_ents = extract_entities(src)
sum_ents = extract_entities(res["summary"])

display(HTML("<h3>Source</h3>" + highlight_entities_html(src, src_ents)))
display(HTML("<h3>Summary</h3>" + highlight_entities_html(res["summary"], sum_ents)))

test_texts = conll_to_texts("/content/ner_data_quick/test.conll")
df_eval = evaluate_on_texts(test_texts)
df_eval.head()